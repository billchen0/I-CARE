{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "from train_cnn import EEGDataModule, EEGNetAutoencoder, EEGNetAutoencoderModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from util import load_split_ids\n",
    "train_ids, val_ids, test_ids = load_split_ids(Path(\"/home/bc299/icare/artifacts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = Path(\"/media/hdd1/i-care/ten-seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = EEGDataModule(train_ids, val_ids, path_to_data)\n",
    "model = EEGNetAutoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2048, 1, 19, 1000]), torch.Size([2048, 1, 19, 1000]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.setup()\n",
    "train_sample = next(iter(dm.train_dataloader()))\n",
    "val_sample = next(iter(dm.val_dataloader()))\n",
    "train_sample.shape, val_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, dtype=torch.uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.isnan(train_sample).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 5 is not equal to len(dims) = 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/bc299/icare/cnn-feature-extraction/test.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bwil.egr.duke.edu/home/bc299/icare/cnn-feature-extraction/test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bwil.egr.duke.edu/home/bc299/icare/cnn-feature-extraction/test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m train_sample \u001b[39m=\u001b[39m train_sample\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mpermute(\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m2\u001b[39;49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 5 is not equal to len(dims) = 4"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "train_sample = train_sample.unsqueeze(1).permute(0, 1, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal = nn.Conv2d(1, 8, kernel_size=[19, 64], stride=1, bias=False, padding=\"same\")\n",
    "bn1 = nn.BatchNorm2d(8)\n",
    "spatial = nn.Conv2d(8, 8*2, kernel_size=[19, 1], bias=False, groups=8)\n",
    "bn2 = nn.BatchNorm2d(16)\n",
    "elu = nn.ELU(True)\n",
    "sepconv1 = nn.Conv2d(16, 16, kernel_size=[1, 16], padding=\"same\", groups=16, bias=False)\n",
    "sepconv2 = nn.Conv2d(16, 16, kernel_size=[1, 1], padding=\"same\", groups=1, bias=False)\n",
    "bn3 = nn.BatchNorm2d(16)\n",
    "pool = nn.AvgPool2d([1, 5], stride=[1, 5], padding=0)\n",
    "\n",
    "encoded = pool(elu(bn3(sepconv2(sepconv1(pool(elu(bn2(spatial(bn1(temporal(train_sample)))))))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "upsamp = nn.Upsample(scale_factor=(1, 5), mode=\"nearest\")\n",
    "desep1 = nn.ConvTranspose2d(16, 16, kernel_size=[1, 16], groups=16, bias=False)\n",
    "desep2 = nn.ConvTranspose2d(16, 16, kernel_size=[1, 1], groups=1, bias=False)\n",
    "dbn1 = nn.BatchNorm2d(16)\n",
    "de_spatial = nn.ConvTranspose2d(16, 8, kernel_size=[19, 1], groups=8, bias=False)\n",
    "de_temporal = nn.ConvTranspose2d(8, 1, kernel_size=[1, 64], stride=1, bias=False)\n",
    "\n",
    "decoded = de_temporal(de_spatial(upsamp(elu(dbn1(desep2(desep1(upsamp(encoded))[:, :, :, :200]))))))[:, :, :, :1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 19, 1000])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 1000, 19])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    model(train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.94972036340321"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "path_to_data = Path(\"/media/hdd1/i-care/ten-seconds/0284/0284_004_008_EEG.npy\")\n",
    "import numpy as np\n",
    "np.max(np.load(path_to_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bc299/miniconda3/envs/icare/lib/python3.11/site-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1003.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'model.png'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "\n",
    "model = EEGNetAutoencoderModel().cuda()\n",
    "x = torch.randn(32, 1, 19, 1000).to(next(model.parameters()).device)\n",
    "y = model(x)\n",
    "make_dot(y, params=dict(model.named_parameters())).render(\"model\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "p = Path(\"/media/hdd1/i-care/five-minutes/0313/0313_018_000_EEG.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 19)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(p).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icare",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
